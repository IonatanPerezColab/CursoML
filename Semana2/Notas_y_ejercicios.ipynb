{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Curso ML - Semana 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPqbdcazHw8gKQuXYeN8Cpx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IonatanPerezColab/CursoML/blob/main/Semana2/Notas_y_ejercicios.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rs1bd0dcpIo3"
      },
      "source": [
        "# 2.1 - Regresiones lineales\n",
        "\n",
        "## Convenciones y definiciones\n",
        "- Si hay mas de una caracterisitca de cada dato se usa la notacion $x_j^i$ donde j hace referencia al tipo de caracteristica del elemento del dataset e i hace referencia al numero de elemento del dataset (vector de datos que corresponden a una misma muestra que pertenece a $\\mathbb{R}^n$).\n",
        "- Se suele llamar \"n\" a la dimencionalidad de las caracteriticas que tiene cada entrada o dato del dataset. Recordar que para el numero de datos usabamos la letra \"m\".\n",
        "- mean normalization: Correr el valor promedio para que sea cero y normlizar (son criterios para acelerar convergencia no tienen consecuencias exactas por eso la eleccion es flexible) $$ x_i = \\frac{(x_i-\\mu)}{(x_{max}-x_{min})/\\sigma}$$\n",
        "\n",
        "- Ecuación normal, solucion analitica a la minimazacion de $J(\\theta)$. Ecuación de derivadas parciales.  Haciendo magia que me gustaria entender (hable con fran y sale de definir la norma de $|\\hat y - y_i|$ pero no se si puedo reconstruirlo), se llega derivando respecto $\\theta$ a que $\\theta = (X^T X)^{-1} X^T y$. Me gustaria rastrear porque pasa esto! Segun fran si no hay caracteristicas que en el dataset sean LD eso asegura que la matriz $X^TX$ es definida positiva y entonces tiene inversa. Pero si hay elementos LD eso puede ser un problema porque se podria eliminar los elementos LD pero hay ahi una arbitrariedad que es importante interpretar. \n",
        "\n",
        "- Al tratar de invertir $X^TX$ puede no ser inversible porque hay elementos LD o porque hay mas caracteristicas que muestras.\n",
        "\n",
        "## Ideas\n",
        "\n",
        "- Si un modelo depende de mas de una caracteritica, el modelo va a tener una variable vectorial\n",
        "- En un modelo multivariado que depende de n caracteristicas se puede escribir la hipotesis como $h(x) = \\theta_0 + \\theta_1 \\cdot x_1 + \\theta_2 \\cdot x_2 + ... + \\theta_n \\cdot x_n$ lo cual se puede escribir como $h(X) = \\Theta^T \\cdot X$ con $x_0 = 1$ donde $X,\\Theta \\in \\mathbb{R}^{n+1}$ \n",
        "- El gradiente para hacer un algoritmo de descenso como en el caso de una variable queda igual (hay que agregar un subindice por formalidad) porque las derivadas de $\\theta_i$ respecto a $x_j$ da cero salvo que $i=j$. \n",
        "\n",
        "- Queda que \n",
        "$$ \\theta_j(t+1) = \\theta_j(t) - \\alpha \\frac{1}{m} \\sum_{i=1}^m (h_{\\theta(t)}(x^i) - y^i) \\cdot x_j^i$$\n",
        "con j en $[0;n]$\n",
        "\n",
        "- Para testear algoritmos de convergencia es util ver como evoluciona $J(\\theta_i)$ vs $i$ donde $\\theta_i$ es el resultado de modificar $\\theta$ luego de la iteración i. La idea es que la función deberia ser decreciente con mucha pendiente al principio y aplanarse despues. \n",
        "\n",
        "- Una manera de evaluar si el algoritmo de convergencia convergio es evaluar $J(\\theta_{i-1}) - J(\\theta_i)$ y ver si es menor que un $\\epsilon$. Pero no es facil elegir el $\\epsilon$, a veces es mas facil ver la forma del grafico.\n",
        "\n",
        "- Otro metaparametro (parametro del algoritmo y no del ajuste de los datos \\theta) a ajustar es el $\\alpha$. En general si hay funciones que son muy concavas y el $\\alpha$ es grande puede \"oscilar\" o ser una función creciente. En cambio si $\\alpha$ es muy chico respecto a la concavidad puede converger muy lento. \n",
        "\n",
        "- Se puede armar polinomios u otra funcion en una variable y es equivalente a un tratamiento multivariado. Es importante normalizar cuando se empiezan a hacer cuentas porque los rango de valores se pueden ir al diablo sino. \n",
        "\n",
        "- Comparación gradiente descendiente vs ecuación normal. Ecuación normal es mucho mas robusto excepto que escala como $n^3$ cuando hay que calcular la inversa de $X^TX$, ahi se vuelve un problema y conviene usar gradiente descendiente. Otro lugar donde no funciona ecuación normal es si los algortimos o modelos no son lineales. \n",
        "\n",
        "## Buenas prácticas\n",
        "\n",
        "- En un espacio multivariado (dataset con muchas caracteristicas-features) conviene normalizar las variables, para que los algoritmos de convergencia sean mas más rápidos. \n",
        "- Hacer mean normzalization: desplazar la variable para que el valor medio este en cero ademas de normalizar. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIjrc09lpWcE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e26de21-9f62-4d22-c4fd-e8e029e87b47"
      },
      "source": [
        "# Cuentas ejercicio \n",
        "\n",
        "import numpy as np\n",
        "\n",
        "datos_x2 = [7921,5184,8836,4761]\n",
        "avg = np.mean(datos_x2)\n",
        "print (avg)\n",
        "rango = max(datos_x2)-min(datos_x2)\n",
        "print (rango)\n",
        "print ((4761-avg)/rango)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6675.5\n",
            "4075\n",
            "-0.4698159509202454\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4orU080DE1Ku"
      },
      "source": [
        "# Ejercitación final semana 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aL7OmVGvFvwO"
      },
      "source": [
        "# Ejercicio 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlEoSX1gGWWg",
        "outputId": "150c3660-a468-4c04-b74f-c4e5a60737a7"
      },
      "source": [
        "# Armamos una matriz identidad de 5x5\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "np.eye(5)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0.],\n",
              "       [0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 0., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    }
  ]
}