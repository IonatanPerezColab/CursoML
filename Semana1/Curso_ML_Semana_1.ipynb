{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Curso ML - Semana 1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IonatanPerezColab/CursoML/blob/main/Semana1/Curso_ML_Semana_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuwYjHx8AbH4"
      },
      "source": [
        "# Cosas generales\n",
        "\n",
        "## Glosario\n",
        "\n",
        "- Supervisado: Que hay data de ejemplos con rta.\n",
        "- Clasificador: Que el output es categorico. Es caso de supervidado.\n",
        "- Regresion: Que el output es continuo. Es caso de supervisado\n",
        "- No supervisado: No hay etiquetas previas en los datos, hay que separar en funcion de ubicación en espacio de fases, clustering\n",
        "- Training set: Datos que se usan para entrenar. Conjunto de m valores input x con su valor de salida y. m pares (x,y) donde x puede pertenecer a $R^n$ y lo mismo y.  \n",
        "- Parametros del modelo: Valores $\\theta_i$ del modelo h(x)\n",
        "- Funcion costo cuadratica: $\\sum_1^m \\frac{1}{2m}\\cdot(h_\\theta(x_i) - y_i)^2 $\n",
        "- Gradiente descendiente: algoritmo de optimización que usa el gradiente de la función costos respecto de los parametros $\\theta$ para moverse en el espacio de parametros y minimizar la funcion costo. \n",
        "\n",
        "## Convenciones de nombres del curso\n",
        "- x: input del modelo supervisado\n",
        "- y: output del modelo supervisado\n",
        "- m: Cantidad de entradas del dataset. \n",
        "- h: modelo que tiene por input a un x y como salida a un valor y: h(x) = y\n",
        "- $h_\\theta$ : modelo que depende de los parametros $\\theta_i$\n",
        "- $J(h,\\theta)$: funcion costo. $\\frac{1}{2} \\sum_i (\\hat y_i - y_i)^2$\n",
        "\n",
        "\n",
        "## Ideas\n",
        "- Cuadrados minimos: minizar la expresion $(h_\\theta(x) - y)^2$\n",
        "- Funcion costo cuadratica: Se elije con norma dos porque suele funcionar bien con los modelos pero es una elección y puede haber otras.\n",
        "- Metodo de gradiente descendiente busca minimos locales. Es inestable frente a condiciones iniciales de prueba.\n",
        "\n",
        "# Notas sobre el curso\n",
        "\n",
        "## Semana 1\n",
        "\n",
        "### Parte 1 : Intro\n",
        "- Trabajan con Octave, voy a tratar de buscar equivalentes en python porque el curso es viejo y probablemente todo lo que no estaba en python en ese momento ya este implementado en librerias. De paso practico buscar cosas en python.\n",
        "\n",
        "### Parte 2 : Funcion costo\n",
        "\n",
        "Arranca con ejemplo de regresion. Modelo lineal $h(x_i) = \\theta_0 + \\theta_1 \\cdot x_i = y_i$\n",
        "\n",
        "### Parte 3 : Algoritmo de gradiente\n",
        "\n",
        "Usan una notacion horrible para las igualdades y las asignaciones. \n",
        "El algortimo gradiente descendiente hace: $\\vec \\theta_i - \\alpha \\nabla J(\\theta_i)$\n",
        "\n",
        "Resulta que $J(\\theta)$ definida como distancias cuadratica es convexa y solo tiene un minimo lo que soluciona el problema de la convergencia a minimos locales. \n",
        "\n",
        "### Parte 4, algebra lineal\n",
        "\n",
        "Notaciones usuales de algebra, vectores, matrices, notacion row/column. Ojo que hay que tener cuidado porque en algebra el primer elemento es el 1, en gral en codigo tipo python es el 0, pero en el curso usan octave! Estuve busando y R usa el 1 tambien parece. \n",
        "\n",
        "Usando notacion matricial, un modelo lineal de la forma $h(x)= y_i = \\theta_0 + \\theta_1 \\cdot x_i + \\theta_2 \\cdot x^2_i ...$ se puede escribir como $Y = X \\cdot \\Theta $ donde X es de la forma $$ X = \\begin{bmatrix}\n",
        "x_1^0 & x_1^1 & x_1^2 & ...\\\\\n",
        "x_2^0 & x_2^1 & x_2^2 & ...\\\\\n",
        "... & ... & ... & ...\n",
        "\\end{bmatrix} $$\n",
        "\n",
        "De la misma forma si queremos evaluar en simultaneo muchos posibles parametros de $\\Theta$ podemos armar una matriz de la forma\n",
        "\n",
        "$$ \\Theta = \\begin{bmatrix}\n",
        "\\theta_0^{h_1} & \\theta_0^{h_2} & ...\\\\\n",
        "\\theta_1^{h_1} & \\theta_1^{h_2} & ...\\\\\n",
        "\\theta_2^{h_1} & \\theta_2^{h_2} & ...\\\\\n",
        "... & ... & ...\n",
        "\\end{bmatrix} $$\n",
        "\n",
        "### Parte 5, algebra lineal\n",
        "\n",
        "Sigue hablando de matrices, propiedades, inversas y transpuestas. No se bien para que presenta este tema pero viene bien de repaso."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bfxJu7wNrEO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10c79680-879f-4584-c8a9-5ac8bcaa0947"
      },
      "source": [
        "\n",
        "# Codigo equivalente a los ejemplos del la semana 1.4.1 (algebra lineal)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "A = np.array([[1,2,3],\n",
        "              [4,5,6],\n",
        "              [7,8,9],\n",
        "              [10,11,12]])\n",
        "v = np.array([1,2,3])\n",
        "m,n = A.shape\n",
        "A_23 = A[1,2] # No encontre que numpy acepte la notiacion empezando con 1\n",
        "print (A_23)\n",
        "\n",
        "# Codigo equivalente a los ejemplos de la semana 1.4.2\n",
        "\n",
        "A = np.array([[1,2,4],\n",
        "              [5,3,2]])\n",
        "\n",
        "B = np.array([[1,2,3],\n",
        "              [1,1,1]])\n",
        "s = 5\n",
        "print (A+B)\n",
        "print (A-B)\n",
        "print (A*s)\n",
        "print (A/s)\n",
        "print ('Lo que sigue se comporta diferente a los previsto con la notacion matematica usual que debería dar error')\n",
        "print (A+s)\n",
        "\n",
        "# Codigo equivalente a los ejemplos de la semana 1.4.3\n",
        "\n",
        "print (\"Inicio de ejemplos de la parte 1.4.3\")\n",
        "A = np.array([[1,2,3],\n",
        "              [4,5,6],\n",
        "              [7,8,9]])\n",
        "v = np.array([1,1,1])\n",
        "print (\"matriz A\")\n",
        "print (A)\n",
        "print (\"vector v\")\n",
        "print (v)\n",
        "print (\"Si ponemos A*v no da lo que uno espera.\")\n",
        "print (A*v)\n",
        "print (\"En cambio funciona si usamos np.dot(A,v)\")\n",
        "print (np.dot(A,v))\n",
        "print ('O bien podemos usar A.dot(v)')\n",
        "print (A.dot(v))\n",
        "\n",
        "# Codigo equivalente a los ejemplos de la semana 1.4.5\n",
        "\n",
        "print (\"\\n Ejemplos 1.4.5 \\n\")\n",
        "A = np.random.randint(10,size = (2,2))\n",
        "B = np.random.randint(10,size = (2,2))\n",
        "I = np.identity(2, dtype=int)\n",
        "print (\"I * A:\")\n",
        "print (I.dot(A))\n",
        "print (\"A * I:\")\n",
        "print (A.dot(I))\n",
        "print (\"B * A:\")\n",
        "print (B.dot(A))\n",
        "print (\"A * B:\")\n",
        "print (A.dot(B))\n",
        "\n",
        "print (\"\\n Ejemplos 1.4.6 \\n\")\n",
        "A = np.array([[1,2,0],\n",
        "              [0,5,6],\n",
        "              [7,0,9]])\n",
        "print (np.transpose(A))\n",
        "print (np.linalg.inv(A))\n",
        "print (np.linalg.inv(A).dot(A))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6\n",
            "[[2 4 7]\n",
            " [6 4 3]]\n",
            "[[0 0 1]\n",
            " [4 2 1]]\n",
            "[[ 5 10 20]\n",
            " [25 15 10]]\n",
            "[[0.2 0.4 0.8]\n",
            " [1.  0.6 0.4]]\n",
            "Lo que sigue se comporta diferente a los previsto con la notacion matematica usual que debería dar error\n",
            "[[ 6  7  9]\n",
            " [10  8  7]]\n",
            "Inicio de ejemplos de la parte 1.4.3\n",
            "matriz A\n",
            "[[1 2 3]\n",
            " [4 5 6]\n",
            " [7 8 9]]\n",
            "vector v\n",
            "[1 1 1]\n",
            "Si ponemos A*v no da lo que uno espera.\n",
            "[[1 2 3]\n",
            " [4 5 6]\n",
            " [7 8 9]]\n",
            "En cambio funciona si usamos np.dot(A,v)\n",
            "[ 6 15 24]\n",
            "O bien podemos usar A.dot(v)\n",
            "[ 6 15 24]\n",
            "\n",
            " Ejemplos 1.4.5 \n",
            "\n",
            "I * A:\n",
            "[[3 5]\n",
            " [7 3]]\n",
            "A * I:\n",
            "[[3 5]\n",
            " [7 3]]\n",
            "B * A:\n",
            "[[27 45]\n",
            " [12 20]]\n",
            "A * B:\n",
            "[[47  0]\n",
            " [75  0]]\n",
            "\n",
            " Ejemplos 1.4.6 \n",
            "\n",
            "[[1 0 7]\n",
            " [2 5 0]\n",
            " [0 6 9]]\n",
            "[[ 0.34883721 -0.13953488  0.09302326]\n",
            " [ 0.3255814   0.06976744 -0.04651163]\n",
            " [-0.27131783  0.10852713  0.03875969]]\n",
            "[[ 1.00000000e+00 -8.32667268e-17  5.55111512e-17]\n",
            " [-2.77555756e-17  1.00000000e+00 -2.77555756e-17]\n",
            " [-3.46944695e-17  2.77555756e-17  1.00000000e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}